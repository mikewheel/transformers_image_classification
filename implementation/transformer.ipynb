{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Vision Transformer in PyTorch\n",
    "## DS4440: Practical Neural Networks Final Project\n",
    "\n",
    "Written by Michael Wheeler, December 2020\n",
    "\n",
    "Original implementation available [here](https://github.com/google-research/vision_transformer).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from sklearn.metrics import accuracy_score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Definitions for model training utility functions."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Taken from the in-class exercise on 2020-10-15 covering image classification with CNNs\n",
    "def train_model(optimizer, model, train_loader, device, num_epochs=5):\n",
    "    losses = []\n",
    "    total_step = len(train_loader)\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # forwards\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # backwards\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # update params\n",
    "            optimizer.step()\n",
    "\n",
    "            if (i + 1) % 100 == 0:\n",
    "                losses.append(loss.item())\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                      .format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))\n",
    "    return losses\n",
    "\n",
    "\n",
    "# Taken from the in-class exercise on 2020-10-15 covering image classification with CNNs\n",
    "def test_model(model, test_loader, device):\n",
    "    # switch to `eval` model\n",
    "    model.eval()\n",
    "\n",
    "    y, y_hat = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_i, y_i in test_loader:\n",
    "            X_i, y_i = X_i.to(device), y_i.to(device)\n",
    "            y_hat_i = model(X_i)\n",
    "            y.extend(y_i.detach().cpu().tolist())\n",
    "            discrete_preds = y_hat_i.argmax(dim=1).detach().cpu()\n",
    "            y_hat.extend(discrete_preds)\n",
    "\n",
    "    return y, y_hat\n",
    "\n",
    "\n",
    "def save_image_from_array(img, path, mode=\"RGB\"):\n",
    "    img = Image.fromarray(img, mode)\n",
    "    img.save(path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Definition of the Vision Transformer class itself."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class VisionTransformer(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, h, w, c, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        # Data dimensions\n",
    "        self.H = h  # Height of an image in pixels\n",
    "        self.W = w  # Width of an image in pixels\n",
    "        self.C = c  # Number of channels of the image (e.g. RGB=3)\n",
    "        self.num_classes = num_classes  # Number of possible classes in the output data\n",
    "\n",
    "        # Model parameters\n",
    "        # See Table 1 of Dosovitsky et al. for other options\n",
    "        self.image_patch_size = 4  # Patch resolution\n",
    "        self.num_patches = int((self.H * self.W) / (self.image_patch_size ** 2))  # Check N\n",
    "        self.transformer_hidden_size = 144  # Embedding dimensions throughout the Transformer\n",
    "        self.transformer_num_layers = 12  # Number of Transformer encoder layers\n",
    "        self.transformer_num_heads = 12  # Number of heads for multi-headed self attention in the Transformer\n",
    "        self.transformer_MLP_size = 512  # dimension of feed-forward layer in the Transformer\n",
    "        self.transformer_activation = \"gelu\"\n",
    "\n",
    "        # Model layers\n",
    "        # Patching and embedding: Done at once in the original implementation\n",
    "        self.patch_embedding_layer = torch.nn.Conv2d(in_channels=self.C, out_channels=self.transformer_hidden_size,\n",
    "                                                     kernel_size=(self.image_patch_size, self.image_patch_size),\n",
    "                                                     stride=(self.image_patch_size, self.image_patch_size))\n",
    "\n",
    "        # Position embedding layer\n",
    "        self.position_embedding_layer = torch.nn.Embedding(num_embeddings=self.num_patches+1,\n",
    "                                                           embedding_dim=self.transformer_hidden_size)\n",
    "\n",
    "        # Transformer itself: out-of-the-box this isn't structured exactly like the original ViT implementation\n",
    "        # TODO explain those differences\n",
    "        self.encoder_layer = torch.nn.TransformerEncoderLayer(d_model=self.transformer_hidden_size,\n",
    "                                                              nhead=self.transformer_num_heads, dropout=0,\n",
    "                                                              dim_feedforward=self.transformer_MLP_size,\n",
    "                                                              activation=self.transformer_activation)\n",
    "        self.encoder = torch.nn.TransformerEncoder(encoder_layer=self.encoder_layer,\n",
    "                                                   num_layers=self.transformer_num_layers)\n",
    "\n",
    "        # The final classification head: \"one hidden layer at pre-training time, linear layer only at fine-tuning time\"\n",
    "        self.classification_head = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=self.transformer_hidden_size, out_features=self.transformer_hidden_size),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(in_features=self.transformer_hidden_size, out_features=self.num_classes),\n",
    "            torch.nn.Softmax(dim=2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        # Patch the images, flatten, and linearly embed: all in one step!\n",
    "        x_patches = self.patch_embedding_layer(x)\n",
    "        # Flatten down to a list of patches instead of a grid\n",
    "        x_patches = x_patches.flatten(start_dim=2)\n",
    "        assert x_patches.shape[2] == self.num_patches\n",
    "\n",
    "        # Zero initialize a class token Tensor\n",
    "        class_token = torch.zeros(*(1, self.transformer_hidden_size, 1))\n",
    "        class_token = class_token.repeat(*(batch_size, 1, 1))\n",
    "        # Concatenate to the beginning of the patch sequence\n",
    "        x_patches_with_class = torch.cat((class_token, x_patches), dim=2)\n",
    "        assert x_patches_with_class.shape[2] == self.num_patches + 1\n",
    "\n",
    "        # Add position embeddings to the sequence\n",
    "        position_embeddings = self.position_embedding_layer(torch.arange(end=self.num_patches+1))\n",
    "        position_embeddings = torch.transpose(*(position_embeddings, 0, 1))\n",
    "        x_patches_with_class_and_posn = x_patches_with_class + position_embeddings\n",
    "        assert x_patches_with_class_and_posn.shape[1] == self.transformer_hidden_size\n",
    "\n",
    "        # Pass the data through the Transformer encoder\n",
    "        x_patches_with_class_and_posn = x_patches_with_class_and_posn.permute(*(2, 0, 1))\n",
    "        transformer_out = self.encoder(x_patches_with_class_and_posn)\n",
    "\n",
    "        # Take from the Transformer's output the learned class token only\n",
    "        transformer_out_class_token = transformer_out[0, :, :].unsqueeze(0)\n",
    "\n",
    "        # Note: Since we're using PyTorch's built-in Transformer encoder an additional LayerNorm is not necessary here\n",
    "        # Pass it through the classification head\n",
    "        out = self.classification_head(transformer_out_class_token)\n",
    "        assert out.shape[2] == 10\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Set up model training hyperparameters:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Global parameters\n",
    "this_device = \"cuda\"\n",
    "num_epochs = 10\n",
    "batch_size = 4096\n",
    "adam_learning_rate = 0.0005\n",
    "adam_betas = (0.9, 0.999)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load the CIFAR-10 data:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load CIFAR-10\n",
    "cifar10_train_dataset = torchvision.datasets.CIFAR10(root=BASE_DIR / \"data\" / \"cifar_10_train\",\n",
    "                                                     train=True, download=True,\n",
    "                                                     transform=torchvision.transforms.ToTensor())\n",
    "cifar10_test_dataset = torchvision.datasets.CIFAR10(root=BASE_DIR / \"data\" / \"cifar_10_test\",\n",
    "                                                    train=False, download=True,\n",
    "                                                    transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "# Make data loaders\n",
    "cifar10_train_data_loader = torch.utils.data.DataLoader(cifar10_train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                                                        num_workers=4)\n",
    "cifar10_test_data_loader = torch.utils.data.DataLoader(cifar10_test_dataset, batch_size=batch_size, shuffle=True,\n",
    "                                                       num_workers=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Initialize the model objects:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get image resolution, num channels\n",
    "assert cifar10_train_dataset.data.shape[1:] == cifar10_test_dataset.data.shape[1:], \\\n",
    "    \"Train and test image dimensions don't match!\"\n",
    "N, H, W, C = cifar10_train_dataset.data.shape\n",
    "assert len(cifar10_train_dataset.classes) == len(cifar10_test_dataset.classes), \\\n",
    "    \"Train and test number of classes don't match!\"  # Sanity check but not strictly necessary\n",
    "cifar10_num_classes = len(cifar10_train_dataset.classes)\n",
    "\n",
    "ViT_model = VisionTransformer(h=H, w=W, c=C, num_classes=cifar10_num_classes)\n",
    "ViT_model.to(this_device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=ViT_model.parameters(), lr=adam_learning_rate, betas=adam_betas)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train the model:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "training_losses = train_model(optimizer, ViT_model, cifar10_train_data_loader, this_device, num_epochs=num_epochs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test the model:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y, y_hat = test_model(ViT_model, cifar10_test_data_loader, this_device)\n",
    "test_accuracy = accuracy_score(y, y_hat)\n",
    "print(f'Test accuracy: {round(test_accuracy, 4)}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}