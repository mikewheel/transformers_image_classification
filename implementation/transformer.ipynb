{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "name": "Final Project: Vision Transformer",
   "provenance": [],
   "collapsed_sections": []
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "7LnDKfyAazPd"
   },
   "source": [
    "# Vision Transformer in PyTorch\n",
    "## DS4440: Practical Neural Networks Final Project\n",
    "\n",
    "Written by Michael Wheeler, December 2020\n",
    "\n",
    "Original implementation available [here](https://github.com/google-research/vision_transformer).\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "4RwfrjuiazPe"
   },
   "source": [
    "import csv\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "from sklearn.metrics import accuracy_score"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "12ZZMz6xazPf"
   },
   "source": [
    "Definitions for model training utility functions."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "JyGzJBGSazPg"
   },
   "source": [
    "# Taken from the in-class exercise on 2020-10-15 covering image classification with CNNs\n",
    "def train_model(optimizer, model, train_loader, device, epochs=5):\n",
    "    # Switch to train\n",
    "    model.train()\n",
    "\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        epoch_losses = []\n",
    "        \n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # forwards\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # backwards\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # update params\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_losses.append(loss.item())\n",
    "        \n",
    "        losses.append(epoch_losses)\n",
    "            \n",
    "    return losses\n",
    "\n",
    "\n",
    "# Taken from the in-class exercise on 2020-10-15 covering image classification with CNNs\n",
    "def test_model(model, test_loader, device):\n",
    "    # Switch to evaluation\n",
    "    model.eval()\n",
    "\n",
    "    y, y_hat = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_i, y_i in test_loader:\n",
    "            X_i, y_i = X_i.to(device), y_i.to(device)\n",
    "            y_hat_i = model(X_i)\n",
    "            y.extend(y_i.detach().cpu().tolist())\n",
    "            discrete_preds = y_hat_i.argmax(dim=1).detach().cpu()\n",
    "            y_hat.extend(discrete_preds)\n",
    "\n",
    "    return y, y_hat\n",
    "\n",
    "\n",
    "def save_image_from_array(img, path, mode=\"RGB\"):\n",
    "    img = Image.fromarray(img, mode)\n",
    "    img.save(path)"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "wQGXlNKrazPg"
   },
   "source": [
    "Definition of the Vision Transformer class itself."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "8qWy8AO1azPg"
   },
   "source": [
    "class VisionTransformer(torch.nn.Module):\n",
    "    \"\"\"PyTorch implementation of the algorithm described in 'An Image is Worth 16x16 Words: Transformers for Image\n",
    "       Recognition At Scale' by Dosovitskiy et al.\n",
    "\n",
    "       Original implementation is available here: https://github.com/google-research/vision_transformer\n",
    "       \"\"\"\n",
    "\n",
    "    def __init__(self, h, w, c, num_classes, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        # Data dimensions\n",
    "        self.H = h  # Height of an image in pixels\n",
    "        self.W = w  # Width of an image in pixels\n",
    "        self.C = c  # Number of channels of the image (e.g. RGB=3)\n",
    "        self.num_classes = num_classes  # Number of possible classes in the output data\n",
    "\n",
    "        # Model parameters\n",
    "        # See Table 1 of Dosovitsky et al. for other options\n",
    "        self.image_patch_size = 4  # Patch resolution\n",
    "        self.num_patches = int((self.H * self.W) / (self.image_patch_size ** 2))  # Check N\n",
    "        self.transformer_hidden_size = 288  # Embedding dimensions throughout the Transformer\n",
    "        self.transformer_num_layers = 12  # Number of Transformer encoder layers\n",
    "        self.transformer_num_heads = 12  # Number of heads for multi-headed self attention in the Transformer\n",
    "        self.transformer_MLP_size = 512  # dimension of feed-forward layer in the Transformer\n",
    "        self.transformer_activation = \"gelu\"\n",
    "        self.dropout_prob = 0.1\n",
    "\n",
    "        # Model layers\n",
    "        # Patching and embedding: Done at once in the original implementation\n",
    "        self.patch_embedding_layer = torch.nn.Conv2d(in_channels=self.C, out_channels=self.transformer_hidden_size,\n",
    "                                                     kernel_size=(self.image_patch_size, self.image_patch_size),\n",
    "                                                     stride=(self.image_patch_size, self.image_patch_size))\n",
    "\n",
    "        # Position embedding layer\n",
    "        self.position_embedding_layer = torch.nn.Embedding(num_embeddings=self.num_patches+1,\n",
    "                                                           embedding_dim=self.transformer_hidden_size)\n",
    "\n",
    "        # Transformer itself: out-of-the-box this isn't structured exactly like the original ViT implementation\n",
    "        # Specifically the application of LayerNorm happens after MSA/MLP (as in Vaswani et al.) instead of before\n",
    "        self.encoder_layer = torch.nn.TransformerEncoderLayer(d_model=self.transformer_hidden_size,\n",
    "                                                              nhead=self.transformer_num_heads, dropout=self.dropout_prob,\n",
    "                                                              dim_feedforward=self.transformer_MLP_size,\n",
    "                                                              activation=self.transformer_activation)\n",
    "        self.encoder = torch.nn.TransformerEncoder(encoder_layer=self.encoder_layer,\n",
    "                                                   num_layers=self.transformer_num_layers)\n",
    "\n",
    "        # The final classification head: \"one hidden layer at pre-training time, linear layer only at fine-tuning time\"\n",
    "        self.classification_head = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=self.transformer_hidden_size, out_features=self.transformer_hidden_size),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(in_features=self.transformer_hidden_size, out_features=self.num_classes),\n",
    "            torch.nn.Softmax(dim=2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        # Patch the images, flatten, and linearly embed: all in one step!\n",
    "        x_patches = self.patch_embedding_layer(x)\n",
    "        # Flatten down to a list of patches instead of a grid\n",
    "        x_patches = x_patches.flatten(start_dim=2)\n",
    "        assert x_patches.shape[2] == self.num_patches\n",
    "\n",
    "        # Zero initialize a class token Tensor\n",
    "        class_token = torch.zeros(*(1, self.transformer_hidden_size, 1),\n",
    "                                  device=self.device)\n",
    "        class_token = class_token.repeat(*(batch_size, 1, 1))\n",
    "        # Concatenate to the beginning of the patch sequence\n",
    "        x_patches_with_class = torch.cat((class_token, x_patches), dim=2)\n",
    "        assert x_patches_with_class.shape[2] == self.num_patches + 1\n",
    "\n",
    "        # Add position embeddings to the sequence\n",
    "        position_embeddings = self.position_embedding_layer(torch.arange(end=self.num_patches+1, device=self.device))\n",
    "        position_embeddings = torch.transpose(*(position_embeddings, 0, 1))\n",
    "        x_patches_with_class_and_posn = x_patches_with_class + position_embeddings\n",
    "        assert x_patches_with_class_and_posn.shape[1] == self.transformer_hidden_size\n",
    "\n",
    "        # Pass the data through the Transformer encoder\n",
    "        x_patches_with_class_and_posn = x_patches_with_class_and_posn.permute(*(2, 0, 1))\n",
    "        transformer_out = self.encoder(x_patches_with_class_and_posn)\n",
    "\n",
    "        # Take from the Transformer's output the learned class token only\n",
    "        transformer_out_class_token = transformer_out[0, :, :].unsqueeze(0)\n",
    "\n",
    "        # Note: Since we're using PyTorch's built-in Transformer encoder an additional LayerNorm is not necessary here\n",
    "        # Pass it through the classification head\n",
    "        out = self.classification_head(transformer_out_class_token)\n",
    "        assert out.shape[2] == 10\n",
    "        return torch.squeeze(out)"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Definition of the baseline CNN as a point of comparison."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class BaselineCNN(torch.nn.Module):\n",
    "    \"\"\"Adapted from the example code at this PyTorch tutorial using CIFAR-10:\n",
    "       https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "       \"\"\"\n",
    "\n",
    "    def __init__(self, h, w, c, num_classes, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        # Data dimensions\n",
    "        self.H = h  # Height of an image in pixels\n",
    "        self.W = w  # Width of an image in pixels\n",
    "        self.C = c  # Number of channels of the image (e.g. RGB=3)\n",
    "        self.num_classes = num_classes  # Number of possible classes in the output data\n",
    "\n",
    "        self.conv_1 = torch.nn.Conv2d(in_channels=self.C, out_channels=6, kernel_size=8)\n",
    "        self.pool_1 = torch.nn.MaxPool2d(2, 2)\n",
    "        self.conv_2 = torch.nn.Conv2d(in_channels=6, out_channels=16, kernel_size=4)\n",
    "        self.pool_2 = torch.nn.MaxPool2d(2, 2)\n",
    "        self.linear_1 = torch.nn.Linear(in_features=16 * 5 * 5, out_features=120)\n",
    "        self.linear_2 = torch.nn.Linear(in_features=120, out_features=84)\n",
    "        self.linear_3 = torch.nn.Linear(in_features=84, out_features=self.num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool_1(F.relu(self.conv_1(x)))\n",
    "        x = self.pool_2(F.relu(self.conv_2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.linear_1(x))\n",
    "        x = F.relu(self.linear_2(x))\n",
    "        x = F.softmax(self.linear_3(x))\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "iTFZwwFEazPg"
   },
   "source": [
    "Set up model training hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "ol6TePDqazPh"
   },
   "source": [
    "# Global parameters\n",
    "this_device = \"cpu\"\n",
    "num_epochs = 500\n",
    "batch_size = 512\n",
    "adam_learning_rate = 0.00015\n",
    "adam_betas = (0.9, 0.999)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "UM94L2QQazPh"
   },
   "source": [
    "Load the CIFAR-10 data:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZAxJSk4AazPh",
    "outputId": "f25295e3-ff81-4a5d-9c3f-d3c0a0bfbfb8"
   },
   "source": [
    "# Load CIFAR-10\n",
    "cifar10_train_dataset = torchvision.datasets.CIFAR10(root=\"./data\",\n",
    "                                                     train=True, download=True,\n",
    "                                                     transform=torchvision.transforms.ToTensor())\n",
    "cifar10_test_dataset = torchvision.datasets.CIFAR10(root=\"./data\",\n",
    "                                                    train=False, download=True,\n",
    "                                                    transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "# Make data loaders\n",
    "cifar10_train_data_loader = torch.utils.data.DataLoader(cifar10_train_dataset, \n",
    "                                                        batch_size=batch_size, \n",
    "                                                        shuffle=True)\n",
    "cifar10_test_data_loader = torch.utils.data.DataLoader(cifar10_test_dataset, \n",
    "                                                       batch_size=batch_size, \n",
    "                                                       shuffle=True)\n",
    "\n",
    "# Get image resolution, num channels\n",
    "assert cifar10_train_dataset.data.shape[1:] == cifar10_test_dataset.data.shape[1:], \\\n",
    "    \"Train and test image dimensions don't match!\"\n",
    "N, H, W, C = cifar10_train_dataset.data.shape\n",
    "assert len(cifar10_train_dataset.classes) == len(cifar10_test_dataset.classes), \\\n",
    "    \"Train and test number of classes don't match!\"  # Sanity check but not strictly necessary\n",
    "cifar10_num_classes = len(cifar10_train_dataset.classes)"
   ],
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n",
      "Failed download. Trying https -> http instead. Downloading http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n",
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "4VAbs8rbazPh"
   },
   "source": [
    "Initialize the model objects:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "sjB5s1HfazPi"
   },
   "source": [
    "ViT_model = VisionTransformer(h=H, w=W, c=C, num_classes=cifar10_num_classes, device=this_device)\n",
    "ViT_model.to(ViT_model.device)\n",
    "ViT_optimizer = torch.optim.Adam(params=ViT_model.parameters(), lr=adam_learning_rate, betas=adam_betas)\n",
    "print(f'Number of parameters in ViT: {sum(p.numel() for p in ViT_model.parameters() if p.requires_grad)}')\n",
    "\n",
    "baseline_model = BaselineCNN(h=H, w=W, c=C, num_classes=cifar10_num_classes, device=this_device)\n",
    "baseline_model.to(baseline_model.device)\n",
    "baseline_optimizer = torch.optim.Adam(params=baseline_model.parameters(), lr=adam_learning_rate, betas=adam_betas)\n",
    "print(f'Number of parameters in baseline CNN: {sum(p.numel() for p in baseline_model.parameters() if p.requires_grad)}')"
   ],
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in ViT: 8306250\n",
      "Number of parameters in baseline CNN: 61844\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Set up an experiment to evaluate improvement of test set top-1 accuracy over the course of model training."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def perform_test_over_epochs_experiment(model, optimizer, name, eval_freq=5):\n",
    "    training_losses = []\n",
    "    test_performance_over_epochs = {}\n",
    "\n",
    "    with open(f'{name}_test_set_accuracy_over_epochs.csv', \"w\") as f:\n",
    "            csv_writer = csv.writer(f)\n",
    "            csv_writer.writerow([\"Epoch\", \"Test Set Accuracy\"])\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        print(f'Begin Epoch {epoch}: {datetime.now()}')\n",
    "        training_losses += train_model(optimizer, model,\n",
    "                                       cifar10_train_data_loader, this_device,\n",
    "                                       epochs=1)\n",
    "\n",
    "        if epoch % eval_freq == 0:\n",
    "            y, y_hat = test_model(model, cifar10_test_data_loader, this_device)\n",
    "            test_accuracy = accuracy_score(y, y_hat)\n",
    "\n",
    "            print(f'Test accuracy at epoch {epoch}: {round(test_accuracy, 4)}')\n",
    "            test_performance_over_epochs[epoch] = test_accuracy\n",
    "\n",
    "            # Export the results of the experiment as we go\n",
    "            with open(f'{name}_test_set_accuracy_over_epochs.csv', \"a\") as f:\n",
    "                csv_writer = csv.writer(f)\n",
    "                csv_writer.writerow([epoch, test_accuracy])\n",
    "\n",
    "    return training_losses, test_performance_over_epochs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "ctesXuQ0azPi"
   },
   "source": [
    "Train and evaluate the baseline model:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N7lCnVpTazPi",
    "outputId": "a9363ff8-7028-417f-8218-d7554d85390c"
   },
   "source": [
    "class BaselineCNN(torch.nn.Module):\n",
    "    \"\"\"Adapted from the example code at this PyTorch tutorial using CIFAR-10:\n",
    "       https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "       \"\"\"\n",
    "\n",
    "    def __init__(self, h, w, c, num_classes, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        # Data dimensions\n",
    "        self.H = h  # Height of an image in pixels\n",
    "        self.W = w  # Width of an image in pixels\n",
    "        self.C = c  # Number of channels of the image (e.g. RGB=3)\n",
    "        self.num_classes = num_classes  # Number of possible classes in the output data\n",
    "\n",
    "        self.conv_1 = torch.nn.Conv2d(in_channels=self.C, out_channels=6, kernel_size=8)\n",
    "        self.pool_1 = torch.nn.MaxPool2d(2, 2)\n",
    "        self.conv_2 = torch.nn.Conv2d(in_channels=6, out_channels=16, kernel_size=4)\n",
    "        self.pool_2 = torch.nn.MaxPool2d(2, 2)\n",
    "        self.linear_1 = torch.nn.Linear(in_features=16 * 5 * 5, out_features=120)\n",
    "        self.linear_2 = torch.nn.Linear(in_features=120, out_features=84)\n",
    "        self.linear_3 = torch.nn.Linear(in_features=84, out_features=self.num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool_1(F.relu(self.conv_1(x)))\n",
    "        x = self.pool_2(F.relu(self.conv_2(x)))\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = F.relu(self.linear_1(x))\n",
    "        x = F.relu(self.linear_2(x))\n",
    "        x = F.softmax(self.linear_3(x))\n",
    "        return x\n",
    "\n",
    "baseline_model = BaselineCNN(h=H, w=W, c=C, num_classes=cifar10_num_classes, device=this_device)\n",
    "baseline_model.to(baseline_model.device)\n",
    "baseline_optimizer = torch.optim.Adam(params=baseline_model.parameters(), lr=adam_learning_rate, betas=adam_betas)\n",
    "print(f'Number of parameters in baseline CNN: {sum(p.numel() for p in baseline_model.parameters() if p.requires_grad)}')\n",
    "baseline_results = perform_test_over_epochs_experiment(baseline_model, baseline_optimizer, name=\"baseline\")"
   ],
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in baseline CNN: 61844\n",
      "Begin Epoch 1: 2020-12-11 13:29:10.306199\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-18-b3f7546a1a5f>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     37\u001B[0m \u001B[0mbaseline_optimizer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptim\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mAdam\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mparams\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mbaseline_model\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparameters\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlr\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0madam_learning_rate\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbetas\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0madam_betas\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     38\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf'Number of parameters in baseline CNN: {sum(p.numel() for p in baseline_model.parameters() if p.requires_grad)}'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 39\u001B[0;31m \u001B[0mbaseline_results\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mperform_test_over_epochs_experiment\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbaseline_model\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbaseline_optimizer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"baseline\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     40\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-14-92ed495a13fe>\u001B[0m in \u001B[0;36mperform_test_over_epochs_experiment\u001B[0;34m(model, optimizer, name, eval_freq)\u001B[0m\n\u001B[1;32m      9\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0mepoch\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnum_epochs\u001B[0m\u001B[0;34m+\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     10\u001B[0m         \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf'Begin Epoch {epoch}: {datetime.now()}'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 11\u001B[0;31m         training_losses += train_model(optimizer, model,\n\u001B[0m\u001B[1;32m     12\u001B[0m                                        \u001B[0mcifar10_train_data_loader\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mthis_device\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m                                        epochs=1)\n",
      "\u001B[0;32m<ipython-input-2-c7305959109b>\u001B[0m in \u001B[0;36mtrain_model\u001B[0;34m(optimizer, model, train_loader, device, epochs)\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     14\u001B[0m             \u001B[0;31m# forwards\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 15\u001B[0;31m             \u001B[0moutputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mimages\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     16\u001B[0m             \u001B[0mloss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcriterion\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlabels\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/ds4440/transformers_image_classification/virtualenv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m    725\u001B[0m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    726\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 727\u001B[0;31m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    728\u001B[0m         for hook in itertools.chain(\n\u001B[1;32m    729\u001B[0m                 \u001B[0m_global_forward_hooks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-18-b3f7546a1a5f>\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     27\u001B[0m         \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpool_1\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrelu\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconv_1\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     28\u001B[0m         \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpool_2\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrelu\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconv_2\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 29\u001B[0;31m         \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mview\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m16\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0;36m5\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0;36m5\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     30\u001B[0m         \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrelu\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlinear_1\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     31\u001B[0m         \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrelu\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlinear_2\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-18-b3f7546a1a5f>\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     27\u001B[0m         \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpool_1\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrelu\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconv_1\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     28\u001B[0m         \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpool_2\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrelu\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconv_2\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 29\u001B[0;31m         \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mview\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m16\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0;36m5\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0;36m5\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     30\u001B[0m         \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrelu\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlinear_1\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     31\u001B[0m         \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrelu\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlinear_2\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Programs/pycharm-2020.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_frame.py\u001B[0m in \u001B[0;36mtrace_dispatch\u001B[0;34m(self, frame, event, arg)\u001B[0m\n\u001B[1;32m    745\u001B[0m                 \u001B[0;31m# if thread has a suspend flag, we suspend with a busy wait\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    746\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0minfo\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpydev_state\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mSTATE_SUSPEND\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 747\u001B[0;31m                     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdo_wait_suspend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mthread\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mframe\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mevent\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0marg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    748\u001B[0m                     \u001B[0;31m# No need to reset frame.f_trace to keep the same trace function.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    749\u001B[0m                     \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrace_dispatch\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Programs/pycharm-2020.1/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_frame.py\u001B[0m in \u001B[0;36mdo_wait_suspend\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    142\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    143\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdo_wait_suspend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 144\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_args\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdo_wait_suspend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    145\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    146\u001B[0m     \u001B[0;31m# IFDEF CYTHON\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Programs/pycharm-2020.1/plugins/python/helpers/pydev/pydevd.py\u001B[0m in \u001B[0;36mdo_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[1;32m   1110\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1111\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_threads_suspended_single_notification\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnotify_thread_suspended\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mthread_id\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstop_reason\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1112\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_do_wait_suspend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mthread\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mframe\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mevent\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0marg\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msuspend_type\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfrom_this_thread\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1113\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1114\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_do_wait_suspend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mthread\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mframe\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mevent\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0marg\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msuspend_type\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfrom_this_thread\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Programs/pycharm-2020.1/plugins/python/helpers/pydev/pydevd.py\u001B[0m in \u001B[0;36m_do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[1;32m   1125\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1126\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprocess_internal_commands\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1127\u001B[0;31m                 \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msleep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0.01\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1128\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1129\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcancel_async_evaluation\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mget_current_thread_id\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mthread\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mid\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mframe\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train and evaluate the ViT model:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ViT_results = perform_test_over_epochs_experiment(ViT_model, ViT_optimizer, name=\"ViT\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}