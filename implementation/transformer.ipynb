{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "7LnDKfyAazPd",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Vision Transformer in PyTorch\n",
    "## DS4440: Practical Neural Networks Final Project\n",
    "\n",
    "Written by Michael Wheeler, December 2020\n",
    "\n",
    "Original implementation available [here](https://github.com/google-research/vision_transformer).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4RwfrjuiazPe",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12ZZMz6xazPf",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Definitions for model training utility functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "JyGzJBGSazPg",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Taken from the in-class exercise on 2020-10-15 covering image classification with CNNs\n",
    "def train_model(optimizer, model, train_loader, device, epochs=5):\n",
    "    # Switch to train\n",
    "    model.train()\n",
    "\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        epoch_losses = []\n",
    "        \n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # forwards\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # backwards\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # update params\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_losses.append(loss.item())\n",
    "        \n",
    "        losses.append(epoch_losses)\n",
    "            \n",
    "    return losses\n",
    "\n",
    "\n",
    "# Taken from the in-class exercise on 2020-10-15 covering image classification with CNNs\n",
    "def test_model(model, test_loader, device):\n",
    "    # Switch to evaluation\n",
    "    model.eval()\n",
    "\n",
    "    y, y_hat = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_i, y_i in test_loader:\n",
    "            X_i, y_i = X_i.to(device), y_i.to(device)\n",
    "            y_hat_i = model(X_i)\n",
    "            y.extend(y_i.detach().cpu().tolist())\n",
    "            discrete_preds = y_hat_i.argmax(dim=1).detach().cpu()\n",
    "            y_hat.extend(discrete_preds)\n",
    "\n",
    "    return y, y_hat\n",
    "\n",
    "\n",
    "def save_image_from_array(img, path, mode=\"RGB\"):\n",
    "    img = Image.fromarray(img, mode)\n",
    "    img.save(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wQGXlNKrazPg"
   },
   "source": [
    "Definition of the Vision Transformer class itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "8qWy8AO1azPg",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class VisionTransformer(torch.nn.Module):\n",
    "    \"\"\"PyTorch implementation of the algorithm described in 'An Image is Worth 16x16 Words: Transformers for Image\n",
    "       Recognition At Scale' by Dosovitskiy et al.\n",
    "\n",
    "       Original implementation is available here: https://github.com/google-research/vision_transformer\n",
    "       \"\"\"\n",
    "\n",
    "    def __init__(self, h, w, c, num_classes, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        # Data dimensions\n",
    "        self.H = h  # Height of an image in pixels\n",
    "        self.W = w  # Width of an image in pixels\n",
    "        self.C = c  # Number of channels of the image (e.g. RGB=3)\n",
    "        self.num_classes = num_classes  # Number of possible classes in the output data\n",
    "\n",
    "        # Model parameters\n",
    "        # See Table 1 of Dosovitsky et al. for other options\n",
    "        self.image_patch_size = 4  # Patch resolution\n",
    "        self.num_patches = int((self.H * self.W) / (self.image_patch_size ** 2))  # Check N\n",
    "        self.transformer_hidden_size = 288  # Embedding dimensions throughout the Transformer\n",
    "        self.transformer_num_layers = 12  # Number of Transformer encoder layers\n",
    "        self.transformer_num_heads = 12  # Number of heads for multi-headed self attention in the Transformer\n",
    "        self.transformer_MLP_size = 512  # dimension of feed-forward layer in the Transformer\n",
    "        self.transformer_activation = \"gelu\"\n",
    "        self.dropout_prob = 0.1\n",
    "\n",
    "        # Model layers\n",
    "        # Patching and embedding: Done at once in the original implementation\n",
    "        self.patch_embedding_layer = torch.nn.Conv2d(in_channels=self.C, out_channels=self.transformer_hidden_size,\n",
    "                                                     kernel_size=(self.image_patch_size, self.image_patch_size),\n",
    "                                                     stride=(self.image_patch_size, self.image_patch_size))\n",
    "\n",
    "        # Position embedding layer\n",
    "        self.position_embedding_layer = torch.nn.Embedding(num_embeddings=self.num_patches+1,\n",
    "                                                           embedding_dim=self.transformer_hidden_size)\n",
    "\n",
    "        # Transformer itself: out-of-the-box this isn't structured exactly like the original ViT implementation\n",
    "        # Specifically the application of LayerNorm happens after MSA/MLP (as in Vaswani et al.) instead of before\n",
    "        self.encoder_layer = torch.nn.TransformerEncoderLayer(d_model=self.transformer_hidden_size,\n",
    "                                                              nhead=self.transformer_num_heads, dropout=self.dropout_prob,\n",
    "                                                              dim_feedforward=self.transformer_MLP_size,\n",
    "                                                              activation=self.transformer_activation)\n",
    "        self.encoder = torch.nn.TransformerEncoder(encoder_layer=self.encoder_layer,\n",
    "                                                   num_layers=self.transformer_num_layers)\n",
    "\n",
    "        # The final classification head: \"one hidden layer at pre-training time, linear layer only at fine-tuning time\"\n",
    "        self.classification_head = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=self.transformer_hidden_size, out_features=self.transformer_hidden_size),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(in_features=self.transformer_hidden_size, out_features=self.num_classes),\n",
    "            torch.nn.Softmax(dim=2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        # Patch the images, flatten, and linearly embed: all in one step!\n",
    "        x_patches = self.patch_embedding_layer(x)\n",
    "        # Flatten down to a list of patches instead of a grid\n",
    "        x_patches = x_patches.flatten(start_dim=2)\n",
    "        assert x_patches.shape[2] == self.num_patches\n",
    "\n",
    "        # Zero initialize a class token Tensor\n",
    "        class_token = torch.zeros(*(1, self.transformer_hidden_size, 1),\n",
    "                                  device=self.device)\n",
    "        class_token = class_token.repeat(*(batch_size, 1, 1))\n",
    "        # Concatenate to the beginning of the patch sequence\n",
    "        x_patches_with_class = torch.cat((class_token, x_patches), dim=2)\n",
    "        assert x_patches_with_class.shape[2] == self.num_patches + 1\n",
    "\n",
    "        # Add position embeddings to the sequence\n",
    "        position_embeddings = self.position_embedding_layer(torch.arange(end=self.num_patches+1, device=self.device))\n",
    "        position_embeddings = torch.transpose(*(position_embeddings, 0, 1))\n",
    "        x_patches_with_class_and_posn = x_patches_with_class + position_embeddings\n",
    "        assert x_patches_with_class_and_posn.shape[1] == self.transformer_hidden_size\n",
    "\n",
    "        # Pass the data through the Transformer encoder\n",
    "        x_patches_with_class_and_posn = x_patches_with_class_and_posn.permute(*(2, 0, 1))\n",
    "        transformer_out = self.encoder(x_patches_with_class_and_posn)\n",
    "\n",
    "        # Take from the Transformer's output the learned class token only\n",
    "        transformer_out_class_token = transformer_out[0, :, :].unsqueeze(0)\n",
    "\n",
    "        # Note: Since we're using PyTorch's built-in Transformer encoder an additional LayerNorm is not necessary here\n",
    "        # Pass it through the classification head\n",
    "        out = self.classification_head(transformer_out_class_token)\n",
    "        assert out.shape[2] == 10\n",
    "        return torch.squeeze(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Definition of the baseline CNN as a point of comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class BaselineCNN(torch.nn.Module):\n",
    "    \"\"\"Adapted from the example code at this PyTorch tutorial using CIFAR-10:\n",
    "       https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "       \"\"\"\n",
    "\n",
    "    def __init__(self, h, w, c, num_classes, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        # Data dimensions\n",
    "        self.H = h  # Height of an image in pixels\n",
    "        self.W = w  # Width of an image in pixels\n",
    "        self.C = c  # Number of channels of the image (e.g. RGB=3)\n",
    "        self.num_classes = num_classes  # Number of possible classes in the output data\n",
    "\n",
    "        self.conv_1 = torch.nn.Conv2d(in_channels=self.C, out_channels=6, kernel_size=8)\n",
    "        self.pool_1 = torch.nn.MaxPool2d(2, 2)\n",
    "        self.conv_2 = torch.nn.Conv2d(in_channels=6, out_channels=16, kernel_size=4)\n",
    "        self.pool_2 = torch.nn.MaxPool2d(2, 2)\n",
    "        self.linear_1 = torch.nn.Linear(in_features=16 * 4 * 4, out_features=128)\n",
    "        self.linear_2 = torch.nn.Linear(in_features=128, out_features=64)\n",
    "        self.linear_3 = torch.nn.Linear(in_features=64, out_features=self.num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool_1(F.relu(self.conv_1(x)))\n",
    "        x = self.pool_2(F.relu(self.conv_2(x)))\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = F.relu(self.linear_1(x))\n",
    "        x = F.relu(self.linear_2(x))\n",
    "        x = F.softmax(self.linear_3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iTFZwwFEazPg",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Set up model training hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ol6TePDqazPh",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Global parameters\n",
    "this_device = \"cpu\"\n",
    "num_epochs = 500\n",
    "batch_size = 512\n",
    "adam_learning_rate = 0.00015\n",
    "adam_betas = (0.9, 0.999)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UM94L2QQazPh"
   },
   "source": [
    "Load the CIFAR-10 data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZAxJSk4AazPh",
    "outputId": "f25295e3-ff81-4a5d-9c3f-d3c0a0bfbfb8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Load CIFAR-10\n",
    "cifar10_train_dataset = torchvision.datasets.CIFAR10(root=\"./data\",\n",
    "                                                     train=True, download=True,\n",
    "                                                     transform=torchvision.transforms.ToTensor())\n",
    "cifar10_test_dataset = torchvision.datasets.CIFAR10(root=\"./data\",\n",
    "                                                    train=False, download=True,\n",
    "                                                    transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "# Make data loaders\n",
    "cifar10_train_data_loader = torch.utils.data.DataLoader(cifar10_train_dataset, \n",
    "                                                        batch_size=batch_size, \n",
    "                                                        shuffle=True)\n",
    "cifar10_test_data_loader = torch.utils.data.DataLoader(cifar10_test_dataset, \n",
    "                                                       batch_size=batch_size, \n",
    "                                                       shuffle=True)\n",
    "\n",
    "# Get image resolution, num channels\n",
    "assert cifar10_train_dataset.data.shape[1:] == cifar10_test_dataset.data.shape[1:], \\\n",
    "    \"Train and test image dimensions don't match!\"\n",
    "N, H, W, C = cifar10_train_dataset.data.shape\n",
    "assert len(cifar10_train_dataset.classes) == len(cifar10_test_dataset.classes), \\\n",
    "    \"Train and test number of classes don't match!\"  # Sanity check but not strictly necessary\n",
    "cifar10_num_classes = len(cifar10_train_dataset.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4VAbs8rbazPh",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Initialize the model objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "sjB5s1HfazPi",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in ViT: 8306250\n",
      "Number of parameters in baseline CNN: 44512\n"
     ]
    }
   ],
   "source": [
    "ViT_model = VisionTransformer(h=H, w=W, c=C, num_classes=cifar10_num_classes, device=this_device)\n",
    "ViT_model.to(ViT_model.device)\n",
    "ViT_optimizer = torch.optim.Adam(params=ViT_model.parameters(), lr=adam_learning_rate, betas=adam_betas)\n",
    "print(f'Number of parameters in ViT: {sum(p.numel() for p in ViT_model.parameters() if p.requires_grad)}')\n",
    "\n",
    "baseline_model = BaselineCNN(h=H, w=W, c=C, num_classes=cifar10_num_classes, device=this_device)\n",
    "baseline_model.to(baseline_model.device)\n",
    "baseline_optimizer = torch.optim.Adam(params=baseline_model.parameters(), lr=adam_learning_rate, betas=adam_betas)\n",
    "print(f'Number of parameters in baseline CNN: {sum(p.numel() for p in baseline_model.parameters() if p.requires_grad)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up an experiment to evaluate improvement of test set top-1 accuracy over the course of model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def perform_test_over_epochs_experiment(model, optimizer, name, eval_freq=5):\n",
    "    training_losses = []\n",
    "    test_performance_over_epochs = {}\n",
    "\n",
    "    with open(f'{name}_test_set_accuracy_over_epochs.csv', \"w\") as f:\n",
    "            csv_writer = csv.writer(f)\n",
    "            csv_writer.writerow([\"Epoch\", \"Test Set Accuracy\"])\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        print(f'Begin Epoch {epoch}: {datetime.now()}')\n",
    "        training_losses += train_model(optimizer, model,\n",
    "                                       cifar10_train_data_loader, this_device,\n",
    "                                       epochs=1)\n",
    "\n",
    "        if epoch % eval_freq == 0:\n",
    "            y, y_hat = test_model(model, cifar10_test_data_loader, this_device)\n",
    "            test_accuracy = accuracy_score(y, y_hat)\n",
    "\n",
    "            print(f'Test accuracy at epoch {epoch}: {round(test_accuracy, 4)}')\n",
    "            test_performance_over_epochs[epoch] = test_accuracy\n",
    "\n",
    "            # Export the results of the experiment as we go\n",
    "            with open(f'{name}_test_set_accuracy_over_epochs.csv', \"a\") as f:\n",
    "                csv_writer = csv.writer(f)\n",
    "                csv_writer.writerow([epoch, test_accuracy])\n",
    "\n",
    "    return training_losses, test_performance_over_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ctesXuQ0azPi",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Train and evaluate the baseline model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N7lCnVpTazPi",
    "outputId": "a9363ff8-7028-417f-8218-d7554d85390c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "baseline_results = perform_test_over_epochs_experiment(baseline_model, baseline_optimizer, name=\"baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and evaluate the ViT model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ViT_results = perform_test_over_epochs_experiment(ViT_model, ViT_optimizer, name=\"ViT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Produce a plot of test-set performance over epochs for each model:\n",
    "(I switched from CoLab to desktop here, hence the config import and slightly different path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 576x576 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import config\n",
    "\n",
    "baseline_epochs = []\n",
    "baseline_test_acc = []\n",
    "ViT_epochs = []\n",
    "ViT_test_acc = []\n",
    "\n",
    "with open(config.BASE_DIR / \"data\" / \"baseline_test_set_accuracy_over_epochs.csv\", \"r\") as baseline_f, \\\n",
    "    open(config.BASE_DIR / \"data\" / \"ViT_test_set_accuracy_over_epochs.csv\", \"r\") as ViT_f:\n",
    "\n",
    "    baseline_reader = csv.reader(baseline_f)\n",
    "    next(baseline_reader)\n",
    "    for epoch, test_acc in baseline_reader:\n",
    "        baseline_epochs.append(int(float(epoch)))\n",
    "        baseline_test_acc.append(float(test_acc))\n",
    "\n",
    "    ViT_reader = csv.reader(ViT_f)\n",
    "    next(ViT_reader)\n",
    "    for epoch, test_acc in ViT_reader:\n",
    "        ViT_epochs.append(int(float(epoch)))\n",
    "        ViT_test_acc.append(float(test_acc))\n",
    "\n",
    "\n",
    "# https://stackoverflow.com/a/41717533/8857601\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 8)\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(b=True, axis='both')\n",
    "\n",
    "baseline_line = plt.plot(baseline_epochs, baseline_test_acc, 'go', label=\"Baseline CNN\")\n",
    "ViT_line = plt.plot(ViT_epochs, ViT_test_acc, 'bs', label=\"Vision Transformer\")\n",
    "plt.title(\"Top-1 Accuracy on CIFAR-10 Test Set vs Training Epochs\")\n",
    "plt.legend()\n",
    "plt.savefig(config.BASE_DIR / \"test_acc_vs_epochs.png\")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Final Project: Vision Transformer",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}