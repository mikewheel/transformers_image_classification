1_AJALD2F2 1_3WTFJ9E5 1_7MWR3CKH 1_Y9GUJC3W 1_QYRW748I 1_ZCP4955Q 1_WS367DHB 1_CVVL3PZC

@article{dosovitskiy2020animage,
	title = {An {Image} is {Worth} 16x16 {Words}: {Transformers} for {Image} {Recognition} at {Scale}},
	shorttitle = {An {Image} is {Worth} 16x16 {Words}},
	url = {http://arxiv.org/abs/2010.11929},
	abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
	urldate = {2020-11-01},
	journal = {arXiv:2010.11929 [cs]},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.11929},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/michael/Zotero/storage/BQ957L46/Dosovitskiy et al. - 2020 - An Image is Worth 16x16 Words Transformers for Im.pdf:application/pdf;arXiv.org Snapshot:/home/michael/Zotero/storage/2YPWV7YE/2010.html:text/html}
}

@misc{2020googleresearchvisiontransformer,
	title = {google-research/vision\_transformer},
	copyright = {Apache-2.0 License         ,                 Apache-2.0 License},
	url = {https://github.com/google-research/vision_transformer},
	abstract = {Contribute to google-research/vision\_transformer development by creating an account on GitHub.},
	urldate = {2020-11-17},
	publisher = {Google Research},
	month = nov,
	year = {2020},
	note = {original-date: 2020-10-21T12:35:02Z}
}

@article{rawat2017deepconvolutional,
	title = {Deep {Convolutional} {Neural} {Networks} for {Image} {Classification}: {A} {Comprehensive} {Review}.},
	volume = {29},
	issn = {1530-888X 0899-7667},
	doi = {10.1162/NECO_a_00990},
	abstract = {Convolutional neural networks (CNNs) have been applied to visual tasks since the  late 1980s. However, despite a few scattered applications, they were dormant until  the mid-2000s when developments in computing power and the advent of large amounts  of labeled data, supplemented by improved algorithms, contributed to their  advancement and brought them to the forefront of a neural network renaissance that  has seen rapid progression since 2012. In this review, which focuses on the  application of CNNs to image classification tasks, we cover their development, from  their predecessors up to recent state-of-the-art deep learning systems. Along the  way, we analyze (1) their early successes, (2) their role in the deep learning  renaissance, (3) selected symbolic works that have contributed to their recent  popularity, and (4) several improvement attempts by reviewing contributions and  challenges of over 300 publications. We also introduce some of their current trends  and remaining challenges.},
	language = {eng},
	number = {9},
	journal = {Neural computation},
	author = {Rawat, Waseem and Wang, Zenghui},
	month = sep,
	year = {2017},
	pmid = {28599112},
	note = {Place: United States},
	pages = {2352--2449}
}

@inproceedings{vaswani2017attention,
	address = {Red Hook, NY, USA},
	series = {{NIPS}'17},
	title = {Attention is all you need},
	isbn = {978-1-5108-6096-4},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
	urldate = {2020-11-01},
	booktitle = {Proceedings of the 31st {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, {\L }ukasz and Polosukhin, Illia},
	month = dec,
	year = {2017},
	pages = {6000--6010},
	file = {Full Text PDF:/home/michael/Zotero/storage/5FMTSE62/Vaswani et al. - 2017 - Attention is all you need.pdf:application/pdf}
}

@article{wu2020visualtransformers,
	title = {Visual {Transformers}: {Token}-based {Image} {Representation} and {Processing} for {Computer} {Vision}},
	shorttitle = {Visual {Transformers}},
	url = {http://arxiv.org/abs/2006.03677},
	abstract = {Computer vision has achieved great success using standardized image representations -- pixel arrays, and the corresponding deep learning operators -- convolutions. In this work, we challenge this paradigm: we instead (a) represent images as a set of visual tokens and (b) apply visual transformers to find relationships between visual semantic concepts. Given an input image, we dynamically extract a set of visual tokens from the image to obtain a compact representation for high-level semantics. We then use visual transformers to operate over the visual tokens to densely model relationships between them. We find that this paradigm of token-based image representation and processing drastically outperforms its convolutional counterparts on image classification and semantic segmentation. To demonstrate the power of this approach on ImageNet classification, we use ResNet as a convenient baseline and use visual transformers to replace the last stage of convolutions. This reduces the stage's MACs by up to 6.9x, while attaining up to 4.53 points higher top-1 accuracy. For semantic segmentation, we use a visual-transformer-based FPN (VT-FPN) module to replace a convolution-based FPN, saving 6.5x fewer MACs while achieving up to 0.35 points higher mIoU on LIP and COCO-stuff.},
	urldate = {2020-11-01},
	journal = {arXiv:2006.03677 [cs, eess]},
	author = {Wu, Bichen and Xu, Chenfeng and Dai, Xiaoliang and Wan, Alvin and Zhang, Peizhao and Tomizuka, Masayoshi and Keutzer, Kurt and Vajda, Peter},
	month = jul,
	year = {2020},
	note = {arXiv: 2006.03677},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {arXiv Fulltext PDF:/home/michael/Zotero/storage/QUVFL5MJ/Wu et al. - 2020 - Visual Transformers Token-based Image Representat.pdf:application/pdf;arXiv.org Snapshot:/home/michael/Zotero/storage/4W8UXQKF/2006.html:text/html}
}

@inproceedings{chen2020generative,
	address = {Vienna, Austria},
	title = {Generative {Pretraining} from {Pixels}},
	url = {https://cdn.openai.com/papers/Generative_Pretraining_from_Pixels_V1_ICML.pdf},
	abstract = {Inspired by progress in unsupervised representa-tion learning for natural language, we examinewhether similar models can learn useful repre-sentations for images. We train a sequence Trans-former to auto-regressively predict pixels, withoutincorporating knowledge of the 2D input struc-ture. Despite training on low-resolution ImageNetwithout labels, we find that a GPT-2 scale modellearns strong image representations as measuredby linear probing, fine-tuning, and low-data clas-sification. On CIFAR-10, we achieve 96.3\% ac-curacy with a linear probe, outperforming a su-pervised Wide ResNet, and 99.0\% accuracy withfull fine-tuning, matching the top supervised pre-trained  models.   We  are  also  competitive  withself-supervised benchmarks on ImageNet whensubstituting pixels for a VQVAE encoding, achiev-ing 69.0\% top-1 accuracy on a linear probe of ourfeatures.},
	urldate = {2020-11-01},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {Proceedings of Machine Learning Research},
	author = {Chen, Mark and Radford, Alec and Child, Rewon and Wu, Jeff and Jun, Heewoo and Luan, David and Sutskever, Ilya},
	month = jul,
	year = {2020}
}

@misc{cifar10,
	title = {{CIFAR}-10 and {CIFAR}-100 datasets},
	url = {https://www.cs.toronto.edu/~kriz/cifar.html},
	urldate = {2020-10-27}
}

@misc{krizhevsky2009learning,
	title = {Learning {Multiple} {Layers} of {Features} from {Tiny} {Images}},
	abstract = {Groups at MIT and NYU have collected a dataset of millions of tiny colour images from the web. It is, in principle, an excellent dataset for unsupervised training of deep generative models, but previous researchers who have tried this have found it dicult to learn a good set of lters from the images. We show how to train a multi-layer generative model that learns to extract meaningful features which resemble those found in the human visual cortex. Using a novel parallelization algorithm to distribute the work among multiple machines connected on a network, we show how training such a model can be done in reasonable time. A second problematic aspect of the tiny images dataset is that there are no reliable class labels which makes it hard to use for object recognition experiments. We created two sets of reliable labels. The CIFAR-10 set has 6000 examples of each of 10 classes and the CIFAR-100 set has 600 examples of each of 100 non-overlapping classes. Using these labels, we show that object recognition is signicantly improved by pre-training a layer of features on a large set of unlabeled tiny images.},
	language = {en},
	urldate = {2020-12-12},
	author = {Krizhevsky, A.},
	year = {2009},
	file = {Snapshot:/home/michael/Zotero/storage/9AXY758R/5d90f06bb70a0a3dced62413346235c02b1aa086.html:text/html}
}
